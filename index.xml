<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Rizwan Khanüë®üèª‚Äçüíª</title><link>https://rizwan-kh.github.io/</link><description>Recent content on Rizwan Khanüë®üèª‚Äçüíª</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>&lt;a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0&lt;/a></copyright><lastBuildDate>Wed, 08 Feb 2023 20:04:03 -0400</lastBuildDate><atom:link href="https://rizwan-kh.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Reload nginx without restarting the container/pod</title><link>https://rizwan-kh.github.io/posts/2023/02/reload-nginx-without-restarting-the-container/pod/</link><pubDate>Wed, 08 Feb 2023 20:04:03 -0400</pubDate><guid>https://rizwan-kh.github.io/posts/2023/02/reload-nginx-without-restarting-the-container/pod/</guid><description>Introduction Today in a troubleshooting session, one of the pods we were working with had an Angular application exposed via an nginx server and we wanted to change a few settings in the custom-nginx.conf, for which the developer informed us that he would need to rebuild the image and the CI pipeline will take around 10-15mins for the build, scan and deploy.
I suggested two approaches
map custom-nginx.conf as a configmap in the pod restart/reload nginx process We went ahead with the second approach as it was the more time-saving option.</description></item><item><title>Working with dead container</title><link>https://rizwan-kh.github.io/posts/2022/07/working-with-dead-container/</link><pubDate>Sat, 16 Jul 2022 19:39:13 +0400</pubDate><guid>https://rizwan-kh.github.io/posts/2022/07/working-with-dead-container/</guid><description>I came across this question from a team member who wanted to troubleshoot a dead container. I use the below process and I thought why not do a small write-up to help the wider audience?
Usage The process is very simple - to save/commit the dead container to a new image and then start a new container with a sh entrypoint and debug the container. I consider you&amp;rsquo;re using docker, although similar equivalent commands for other container runtime alternatives like podman, ctr, etc.</description></item><item><title>Integration of Azure AD as OIDC identity provider for Kubernetes</title><link>https://rizwan-kh.github.io/posts/2022/03/integration-of-azure-ad-as-oidc-identity-provider-for-kubernetes/</link><pubDate>Mon, 14 Mar 2022 18:23:08 +0400</pubDate><guid>https://rizwan-kh.github.io/posts/2022/03/integration-of-azure-ad-as-oidc-identity-provider-for-kubernetes/</guid><description>Introduction In my project, we are using many flavours of Kubernetes viz. EKS, AKS, GKE, RKE, ACK. RBAC for all these clusters is managed via a central Active Directory as well as the user authentication, and this is achieved centrally by onboarding all the clusters on Rancher to manage all Kubernetes clusters.
I had a requirement where we couldn&amp;rsquo;t onboard the users to our Active Directory, and the plan was to give them access to the Kubernetes cluster via Azure AD external users(or guest users).</description></item><item><title>Hashicorp Vault &amp; Azure AD Identity Integration</title><link>https://rizwan-kh.github.io/posts/2022/02/hashicorp-vault-azure-ad-identity-integration/</link><pubDate>Tue, 15 Feb 2022 19:31:58 +0400</pubDate><guid>https://rizwan-kh.github.io/posts/2022/02/hashicorp-vault-azure-ad-identity-integration/</guid><description>Introduction In this post, we will be implementing Hashicorp Vault authentication with Microsoft Azure AD as an identity provider using OIDC(OpenID Connect). The default token authentication is always enabled.
Configuration There are two configurations that we had to do to achieve this with Azure AD, one is for app registration in the Azure Portal and the other one is on Vault - enabling the OIDC based auth. The steps for both are described below.</description></item><item><title>Integration of Azure AD as OIDC identity provider for AWS EKS</title><link>https://rizwan-kh.github.io/posts/2021/12/integration-of-azure-ad-as-oidc-identity-provider-for-aws-eks/</link><pubDate>Wed, 22 Dec 2021 23:40:21 +0400</pubDate><guid>https://rizwan-kh.github.io/posts/2021/12/integration-of-azure-ad-as-oidc-identity-provider-for-aws-eks/</guid><description>Introduction In my project, we are using many flavours of Kubernetes viz. EKS, AKS, GKE, RKE, ACK. RBAC for all these clusters are managed via a central Active Directory as well as the user authentication, and this is achieved centrally by onboarding all the cluster on Rancher to manage all Kubernetes cluster.
I had a requirement where we couldn&amp;rsquo;t onboard the users to our Active Directory, and the plan was to give them access to Amazon EKS via an Azure AD external users(or guest users).</description></item><item><title>Trivy - Scan Container Images</title><link>https://rizwan-kh.github.io/posts/2021/12/trivy-scan-container-images/</link><pubDate>Tue, 14 Dec 2021 23:10:21 +0400</pubDate><guid>https://rizwan-kh.github.io/posts/2021/12/trivy-scan-container-images/</guid><description>Trivy Trivy is a scanner for vulnerabilities in container images, file systems, git repositories and configuration. It&amp;rsquo;s an Aqua Security open-source project that can be easily used to integrate with our existing CI CD pipeline or used as a stand-alone tool to scan container images deployed on the Kubernetes cluster.
We were using Clair previous to our switch to Trivy as we analyzed both the tools and found Trivy to be more helpful for the longer run - it was faster, had more CVE database updates as compared to Clair and didn&amp;rsquo;t depend on external clients to interact/scan.</description></item><item><title>Traefik on Kubernetes with Let's Encrypt &amp; Route53</title><link>https://rizwan-kh.github.io/posts/2021/07/traefik-on-kubernetes-with-lets-encrypt-route53/</link><pubDate>Wed, 28 Jul 2021 17:40:27 +0400</pubDate><guid>https://rizwan-kh.github.io/posts/2021/07/traefik-on-kubernetes-with-lets-encrypt-route53/</guid><description>Why Traefik? Traefik is a modern dynamic load balancer and reverse proxy, it&amp;rsquo;s easy to set up, and control and provides lots of options which sit right with our use cases. It integrates with Lets Encrypt to provide SSL termination along with support for service discovery, tracing, and metrics out of the box running on Kubernetes as a small pod.
Since when We have been using Traefik since early 2017 on 2 of our clusters as a daemonset Kubernetes object with external DNS to update the worker nodes IP with our DNS provider and used to manually generate and update Lets Encrypt certificate on a Kubernetes secret object.</description></item><item><title>(Azure DevOps) Send Json Request(Parameters) to Azure Pipelines</title><link>https://rizwan-kh.github.io/posts/2021/06/azure-devops-send-json-requestparameters-to-azure-pipelines/</link><pubDate>Mon, 21 Jun 2021 11:16:23 +0400</pubDate><guid>https://rizwan-kh.github.io/posts/2021/06/azure-devops-send-json-requestparameters-to-azure-pipelines/</guid><description>Introduction If you&amp;rsquo;ve been using Azure DevOps, you would know that a pipeline can be triggered with runtime parameters in the format key: value pair and this is great for doing almost all of the tasks.
For our use case, we had been looking at an option to send a JSON-based parameter dictionary and I couldn&amp;rsquo;t find any way at the time of writing this article. We came up with a hack to achieve this and I would want to write it up in this blog post.</description></item><item><title>(Azure DevOps) How to commit/push to Azure Git Repository from Azure Pipeline</title><link>https://rizwan-kh.github.io/posts/2021/03/azure-devops-how-to-commit/push-to-azure-git-repository-from-azure-pipeline/</link><pubDate>Sun, 14 Mar 2021 00:28:21 +0400</pubDate><guid>https://rizwan-kh.github.io/posts/2021/03/azure-devops-how-to-commit/push-to-azure-git-repository-from-azure-pipeline/</guid><description>Azure Git Repository In one of our use-case, we had to create some terraform HCL files via Azure DevOps pipeline operations and then commit/push those newly generated files back to Azure Git Repos. This is generally not a very usual use case or operation. However, we were able to achieve it using the below pipeline.
Assuming, your pipeline has completed and has generated or modified the files, now it&amp;rsquo;s time to push the changes to git - this surely will trigger another pipeline, however, this second pipeline will start the deployment and not the generation of terraform files as it did in the first pipeline trigger.</description></item><item><title>I am Rizwan Khan;</title><link>https://rizwan-kh.github.io/about/</link><pubDate>Wed, 02 Sep 2020 16:44:33 +0400</pubDate><guid>https://rizwan-kh.github.io/about/</guid><description>welcome to my page - I am from Kolkata, a city in India. I grew up in a joint family. I graduated from the West Bengal University of Technology with a major in Information Technology and most part of my education was completed in the same city.
I started my career with CMC Limited in Mumbai and used to work on C++ and C for a major Stock Exchange as a Backend Developer.</description></item></channel></rss>