<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Rizwan Khanüë®üèª‚Äçüíª</title><link>https://rizwan-kh.github.io/posts/</link><description>Recent content in Posts on Rizwan Khanüë®üèª‚Äçüíª</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>&lt;a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0&lt;/a></copyright><lastBuildDate>Mon, 14 Mar 2022 18:23:08 +0400</lastBuildDate><atom:link href="https://rizwan-kh.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Integration of Azure AD as OIDC identity provider for Kubernetes</title><link>https://rizwan-kh.github.io/posts/2022/03/integration-of-azure-ad-as-oidc-identity-provider-for-kubernetes/</link><pubDate>Mon, 14 Mar 2022 18:23:08 +0400</pubDate><guid>https://rizwan-kh.github.io/posts/2022/03/integration-of-azure-ad-as-oidc-identity-provider-for-kubernetes/</guid><description>Introduction In my project, we are using many flavors of Kubernetes viz. EKS, AKS, GKE, RKE, ACK. RBAC for all these clusters is managed via a central Active Directory as well as the user authentication, and this is achieved centrally by onboarding all the clusters on Rancher to manage all Kubernetes clusters.
I had a requirement where we couldn&amp;rsquo;t onboard the users to our Active Directory, and the plan was to give them access to the Kubernetes cluster via Azure AD external users(or guest users).</description><content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>In my project, we are using many flavors of Kubernetes viz. EKS, AKS, GKE, RKE, ACK. RBAC for all these clusters is managed via a central Active Directory as well as the user authentication, and this is achieved centrally by onboarding all the clusters on Rancher to manage all Kubernetes clusters.</p>
<p>I had a requirement where we couldn&rsquo;t onboard the users to our Active Directory, and the plan was to give them access to the Kubernetes cluster via Azure AD external users(or guest users).</p>
<p>OIDC based authentication is natively supported by Kubernetes and we will be taking advantage of this to set up authentication and authorization using Azure AD.</p>
<h2 id="setup">Setup</h2>
<h3 id="setup-azure-ad-app-registration">Setup Azure AD App registration</h3>
<ul>
<li>Click on New Registration</li>
<li>Provide a name viz. <code>k8s-auth-app</code></li>
<li>Select <code>Accounts in this organizational directory only (MyAccount only - Single tenant)</code></li>
<li>Click <code>Register</code></li>
</ul>
<p>After the app is created, there is a couple of configuration that needs to be performed.</p>
<ul>
<li>Click on <code>Authentication</code> and under <code>Advance settings</code> and check the <code>Allow public client flows</code> and save it</li>
</ul>
<hr>
<ul>
<li>check if platform needs to be added and if yes, then add a platform of type Web with redirect URI as <code>http://localhost/red</code> and select <code>ID tokens (used for implicit and hybrid flows)</code></li>
</ul>
<hr>
<ul>
<li>
<p>If you want group to be part of your OIDC, under Token configuration click Add groups claim. Select Security groups and Group ID. Groups created in AAD can only be included by their ObjectID and not name.</p>
</li>
<li>
<p>Copy the <code>Application (client) ID</code> and <code>Directory (tenant) ID</code> to be used later.</p>
</li>
</ul>
<h3 id="configure-kubernetes-api-server">Configure Kubernetes API Server</h3>
<p>Kubernetes provides a way to configure OIDC compatible identity providers via flags passed to the kube-apiserver component. We need to the below flags while starting the kube-apiserver</p>
<pre tabindex="0"><code>--oidc-client-id=&#34;spn:&lt;application id&gt;&#34; \
--oidc-issuer-url=&#34;https://sts.windows.net/&lt;azure AD tenant&gt;/&#34;
--oidc-username-claim=&#34;email&#34; # this will be `upn` if you want to authenticate direct member users of Azure AD and not guest users
</code></pre><p>If you have created your cluster using <a href="https://kops.sigs.k8s.io/">KOPS</a>, you can add the below in the cluster configuration and perform an update and rolling-update to re-create the master nodes to enable the authentication</p>
<pre tabindex="0"><code>spec:
    kubeAPIServer:
        oidcClientID: spn:&lt;application ID&gt;
        oidcIssuerURL: https://sts.windows.net/&lt;azure AD tenant&gt;/
        oidcUsernameClaim: upn
        oidcUsernamePrefix: &#39;aad:&#39;
</code></pre><p>After the master nodes are up and running, the server configuration is completed. We will proceed with configuring clients</p>
<h3 id="client-configuration">Client configuration</h3>
<p>Since mostly, we use <code>kubectl</code> to interact with Kubernetes, we will configure kubectl to use - <a href="https://github.com/Azure/kubelogin">kubelogin</a> which is a <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#client-go-credential-plugins">client-go credential (exec) plugin</a> implementing azure authentication. This plugin provides features that are not available in kubectl. It is supported on kubectl v1.11+</p>
<ul>
<li>azure cli</li>
</ul>
<p>We will explain below on how to configure both</p>
<h4 id="install-azurekubelogin">Install Azure/kubelogin</h4>
<p>I followed the installation instructions from <a href="https://github.com/Azure/kubelogin">https://github.com/Azure/kubelogin</a>:</p>
<p>Install using homebrew:</p>
<pre tabindex="0"><code>brew install Azure/kubelogin/kubelogin
</code></pre><p>Install directly from Github</p>
<pre tabindex="0"><code>wget https://github.com/Azure/kubelogin/releases/latest/download/kubelogin-linux-amd64.zip
unzip kubelogin-linux-amd64.zip -d kubelogin
mv kubelogin/bin/linux_amd64/kubelogin /usr/local/bin/
rm -r kubelogin*
</code></pre><h4 id="install-azure-cli">Install Azure cli</h4>
<p>Install using homebrew</p>
<pre tabindex="0"><code>brew update &amp;&amp; brew install azure-cli
</code></pre><p>Although not preferred by many, using the script we can install as per below</p>
<pre tabindex="0"><code>curl -L https://aka.ms/InstallAzureCli | bash
</code></pre><h4 id="configure-kubectl">Configure kubectl</h4>
<p>Below kubeconfig contains sample garbage value, please replace the below fields with proper value</p>
<ul>
<li>certificate-authority-data</li>
<li>server</li>
<li>value for server-id</li>
<li>value for client-id</li>
<li>value for tenant-id</li>
</ul>
<pre tabindex="0"><code># using kubelogin
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tC ...REDACTED STRING
    server: https://34DA2A37GFSDXY7GFYWGE7ABA34Q11R0.myk8s.com
  name: k8s-aad
contexts:
- context:
    cluster: k8s-aad
    user: azure-user
  name: k8s-azure-user
current-context: k8s-azure-user
kind: Config
preferences: {}
users:
- name: azure-user
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - get-token
      - --environment
      - AzurePublicCloud
      - --server-id
      - a3xxxx4fe-xxxx-xxxx-xxxx-dexxxxxx210
      - --client-id
      - a3xxxx4fe-xxxx-xxxx-xxxx-dexxxxxx210
      - --tenant-id
      - 67xxx5c-xxxx-xxxx-xxxx-254xxxxx9ccf
      command: kubelogin
      env: null
</code></pre><pre tabindex="0"><code># using azure cli
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tC ...REDACTED STRING
    server: https://34DA2A37GFSDXY7GFYWGE7ABA34Q11R0.myk8s.com
  name: k8s-aad
contexts:
- context:
    cluster: k8s-aad
    user: azure-user
  name: k8s-azure-user
current-context: k8s-azure-user
kind: Config
preferences: {}
users:
- name: azure-user
  user:
    auth-provider:
      config:
        apiserver-id: a3xxxx4fe-xxxx-xxxx-xxxx-dexxxxxx210
        client-id: a3xxxx4fe-xxxx-xxxx-xxxx-dexxxxxx210
        environment: AzurePublicCloud
        tenant-id: 67xxx5c-xxxx-xxxx-xxxx-254xxxxx9ccf
      name: azure
</code></pre><h3 id="authentication">Authentication</h3>
<p>Post completion of this setup, issue <code>kubectl</code> command to get the instruction to authenticate yourself; Note this will only authenticate you, you would need to configure RBAC to allow the users to interact with the cluster.</p>
<pre tabindex="0"><code>kubectl get pods
To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code EJQH9Q8LS to authenticate.
</code></pre><h3 id="authorization">Authorization</h3>
<p>We would need to setup RBAC for different users and group as per our need, for that I created 3 Azure AD groups and mapped then as below</p>
<ol>
<li>k8s-admins</li>
<li>k8s-editors</li>
<li>k8s-viewers</li>
</ol>
<pre tabindex="0"><code>kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  annotations:
    for.ref.AAD.Group: https://portal.azure.com/#blade/Microsoft_AAD_IAM/GroupDetailsMenuBlade/Overview/groupId/abe95a71-b52a-43f2-9095-fecd4f6ef58d
  name: aad-cluster-admins
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: aad:abe95a71-b52a-43f2-9095-fecd4f6ef58d  # group ID setup
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: aad:rizwan.khan@mycontoso.com  # user setup
  
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    for.ref.AAD.Group: https://portal.azure.com/#blade/Microsoft_AAD_IAM/GroupDetailsMenuBlade/Overview/groupId/0bec3baa-511d-4816-ae62-2758d6023cf1
  name: aad-cluster-editors
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: edit
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: aad:0bec3baa-511d-4816-ae62-2758d6023cf1

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  annotations:
    for.ref.AAD.Group: https://portal.azure.com/#blade/Microsoft_AAD_IAM/GroupDetailsMenuBlade/Overview/groupId/03e95a71-b92a-4cf2-9895-fecd3f6ef58d
  name: aad-cluster-viewers
roleRef:
  kind: ClusterRole
  name: view
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: Group
  name: aad:03e95a71-b92a-4cf2-9895-fecd3f6ef58d
  apiGroup: rbac.authorization.k8s.io
</code></pre><p>After these RBAC are set, I added my user and a couple of others in those groups and voila, we were all set and good to go. We can also set up RoleBindings per namespace and access level as per requirement.</p>
]]></content></item><item><title>Hashicorp Vault &amp; Azure AD Identity Integration</title><link>https://rizwan-kh.github.io/posts/2022/02/hashicorp-vault-azure-ad-identity-integration/</link><pubDate>Tue, 15 Feb 2022 19:31:58 +0400</pubDate><guid>https://rizwan-kh.github.io/posts/2022/02/hashicorp-vault-azure-ad-identity-integration/</guid><description>Introduction In this post, we will be implementing Hashicorp Vault authentication with Microsoft Azure AD as an identity provider using OIDC(OpenID Connect). The default token authentication is always enabled.
Configuration There are two configurations that we had to do to achieve this with Azure AD, one is for app registration in the Azure Portal and the other one is on Vault - enabling the OIDC based auth. Steps for both are described below.</description><content type="html"><![CDATA[<p><img src="/vault-auth-basic.webp" alt="OIDC"></p>
<h2 id="introduction">Introduction</h2>
<p>In this post, we will be implementing Hashicorp Vault authentication with Microsoft Azure AD as an identity provider using OIDC(OpenID Connect). The default <strong>token</strong> authentication is always enabled.</p>
<h2 id="configuration">Configuration</h2>
<p>There are two configurations that we had to do to achieve this with Azure AD, one is for app registration in the Azure Portal and the other one is on Vault - enabling the OIDC based auth. Steps for both are described below.</p>
<h3 id="azure-ad-app-registration">Azure AD App Registration</h3>
<p>Navigate to Azure AD in the Azure portal and initiate a new registration under &lsquo;App Registration&rsquo;. Give a name to the app and select web under Redirect URI and add http://localhost:8250/oidc/callback and Register. We need to add more Redirect URI which we will do once the app is created. Copy the Tenant ID and Client ID(we will need those later). Under Authentication, we need to add Redirect URI based on the Vault URL. Create a secret and copy it for later use. We will also need to give permission for Microsoft Graph API Permission for &ldquo;Group.Read.All&rdquo;.</p>
<p>In short, we need the below from Microsoft Azure:</p>
<ul>
<li>Register 1 App under App registration in Azure AD with name vault-aad-auth</li>
<li>web-based Redirect URI for the below URLs
<ul>
<li>http://localhost:8250/oidc/callback</li>
<li>https://localhost:8250/oidc/callback</li>
<li><a href="https://vault.x-ops.com:8200/ui/vault/auth/oidc/oidc/callback">https://vault.x-ops.com:8200/ui/vault/auth/oidc/oidc/callback</a></li>
<li><a href="https://vault.x-ops.com/ui/vault/auth/oidc/oidc/callbac">https://vault.x-ops.com/ui/vault/auth/oidc/oidc/callbac</a></li>
</ul>
</li>
<li>Create a secret and copy the value</li>
<li>Copy the Directory(tenant) ID and Application(client) ID</li>
<li>Microsoft Graph API Permission for Group.Read.All</li>
</ul>
<h3 id="vault-configurations">Vault Configurations</h3>
<h4 id="enable-oidc">enable OIDC</h4>
<p>Enabling the OIDC based auth is the first thing we need to do, if you want to configure multiple OIDC, we can change the path on which OIDC is initialized.  Go to the Access tab at top of the Vault UI page and click on &lsquo;Enable new method +&rsquo; under Auth methods, alternatively, you can use Vault CLI to enable this using the below command</p>
<pre tabindex="0"><code>vault auth enable oidc
</code></pre><h4 id="configure-oidc">configure OIDC</h4>
<p>Post initializing the OIDC authentication, we need to configure OIDC using the below OIDC command, here we will need the Tenant ID, client ID, and secret that we created in the Azure AD App registration step. Replace the values in the below command before applying.</p>
<pre tabindex="0"><code>vault write auth/oidc/config \
oidc_discovery_url=&#34;https://login.microsoftonline.com/TENANT_ID/v2.0&#34; \
oidc_client_id=&#34;CLIENT_ID&#34; \
oidc_client_secret=&#34;SECRET_REDACTED&#34;
</code></pre><h4 id="configure-role">configure role</h4>
<p>Replace the vault URL in the below <em>allowed_redirect_uris</em> as applicable and only keep one among the last twos depending on which port vault is exposed.</p>
<pre tabindex="0"><code>vault write auth/oidc/role/aad \
  user_claim=&#34;email&#34; \
  oidc_scopes=&#34;https://graph.microsoft.com/.default&#34; \
  groups_claim=&#34;groups&#34; \
  policies=default \
  ttl=1h \
  allowed_redirect_uris=&#34;http://localhost:8250/oidc/callback&#34; \
  allowed_redirect_uris=&#34;https://localhost:8250/oidc/callback&#34; \
  allowed_redirect_uris=&#34;https://vault.x-ops.com:8200/ui/vault/auth/oidc/oidc/callback&#34; \
  allowed_redirect_uris=&#34;https://vault.x-ops.com/ui/vault/auth/oidc/oidc/callback&#34;
</code></pre><h4 id="verify-oidc-login-with-above-role">verify OIDC login with above role</h4>
<p>You can now try to log in using the above role via the below command or use the Vault UI with the OIDC method and role <code>aad</code>.</p>
<pre tabindex="0"><code>vault login -method=oidc role=aad
</code></pre><p>You will be authenticated to login but won&rsquo;t be able to see any credentials yet as policies are not yet set, which will be completed as below</p>
<h4 id="create-two-policy-per-team---read-only-and-readwritedelete">create two policy per team - read-only and read/write/delete</h4>
<pre tabindex="0"><code># admin policy for kv backend at path kv/
path &#34;kv/*&#34; {
    capabilities = [&#34;create&#34;, &#34;read&#34;, &#34;update&#34;, &#34;list&#34;, &#34;delete&#34;]
}
</code></pre><pre tabindex="0"><code># read-only policy for kv backend at path kv/
path &#34;kv/*&#34; {
    capabilities = [&#34;read&#34;, &#34;list&#34;]
}
</code></pre><p>Save the two policies in two files named - <code>devops-admin-policy.hcl</code> and <code>devops-ro-policy.hcl</code>. Use the below command to write these policies</p>
<pre tabindex="0"><code>vault policy write team-devops-admin-policy &#34;devops-admin-policy.hcl&#34;
vault policy write team-devops-ro-policy &#34;devops-ro-policy.hcl&#34;
</code></pre><h4 id="create-or-select-azure-ad-groups-mapped-to-the-two-policies-per-team">create or select azure AD groups mapped to the two policies per team</h4>
<p>Create two Azure AD groups and map the corresponding users as applicable and note down the <strong>Object(Group) ID</strong> for both the groups - one will be used for admin users and one for read-only users</p>
<h4 id="create-group-mapping">create group mapping</h4>
<p>Post group creation, we need to perform two tasks</p>
<ul>
<li>create groups in Vault and attach policies</li>
<li>create identity mapping between the AAD groups and vault groups</li>
</ul>
<h5 id="internal-vault-group">internal vault group</h5>
<pre tabindex="0"><code>vault write identity/group name=&#34;devops-admins&#34; type=&#34;external&#34; policies=&#34;team-devops-admin-policy&#34;
vault write identity/group name=&#34;devops-ro&#34; type=&#34;external&#34; policies=&#34;team-devops-ro-policy&#34;
</code></pre><p>Post execution, note down the canonical ID for both the groups which will be used in mapping with</p>
<h5 id="external-aad-group-mapping">external AAD group mapping</h5>
<pre tabindex="0"><code># vault write identity/group-alias name=&#34;&lt;azuread-group-object-id&gt;&#34; mount_accessor=&#34;oidc_mount_accessor&#34; canonical_id=&#34;&lt;canonical-id&gt;&#34;

# for admin group
vault write identity/group-alias name=&#34;7f8a791a-45ac-49a7-8883-406k2pfq844a&#34; mount_accessor=&#34;auth_oidc_a26f03c3&#34; canonical_id=&#34;383344sf-88x5-19s5-da01-9e0011foe9x3&#34;

# for read-only users group
vault write identity/group-alias name=&#34;7f8a791a-3222-cc33-8899-4061122b334a&#34; mount_accessor=&#34;auth_oidc_a26f03c3&#34; canonical_id=&#34;3883336f-8555-1aa5-dww1-9e1122xae9b3&#34;
</code></pre><p>Now you can try to log in with the OIDC and role as <code>aad</code>; Provide you Azure credentials and you will be authentication and if authorized will able to login and view the authorized credentials.</p>
]]></content></item><item><title>Integration of Azure AD as OIDC identity provider for AWS EKS</title><link>https://rizwan-kh.github.io/posts/2021/12/integration-of-azure-ad-as-oidc-identity-provider-for-aws-eks/</link><pubDate>Wed, 22 Dec 2021 23:40:21 +0400</pubDate><guid>https://rizwan-kh.github.io/posts/2021/12/integration-of-azure-ad-as-oidc-identity-provider-for-aws-eks/</guid><description>Introduction In my project we are using many flavours of Kubernetes viz. EKS, AKS, GKE, RKE, ACK. RBAC for all these cluster are managed via a central Active Directory as well as the user authentication, and this is achieved centrally by onboarding all the cluster on Rancher to manage all Kubernetes cluster.
I had a requirement where we couldn&amp;rsquo;t onboard the users to our Active Directory, and the plan was to give them access to Amazon EKS via an Azure AD external users(or guest users).</description><content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>In my project we are using many flavours of Kubernetes viz. EKS, AKS, GKE, RKE, ACK. RBAC for all these cluster are managed via a central Active Directory as well as the user authentication, and this is achieved centrally by onboarding all the cluster on Rancher to manage all Kubernetes cluster.</p>
<p>I had a requirement where we couldn&rsquo;t onboard the users to our Active Directory, and the plan was to give them access to Amazon EKS via an Azure AD external users(or guest users).</p>
<p>Since now, <a href="https://aws.amazon.com/about-aws/whats-new/2021/02/amazon-eks-clusters-support-user-authentication-oidc-compatible-identity-providers/">Amazon supports user authentication with OIDC compatible identity provider</a>, I tried my hands at integrating AAD guest users to access this EKS.</p>
<h2 id="setup">Setup</h2>
<h3 id="setup-azure-ad-app-registration">Setup Azure AD App registration</h3>
<ul>
<li>Click on New Registration</li>
<li>Provide a name viz. <code>eks-auth-app</code></li>
<li>Select <code>Accounts in this organizational directory only (MyAccount only - Single tenant)</code></li>
<li>Click <code>Register</code></li>
</ul>
<p>After the app is created, there are couple of configuration that needs to be performed.</p>
<ul>
<li>Click on <code>Authentication</code> and under <code>Advance settings</code> and check the <code>Allow public client flows</code> and save it</li>
</ul>
<hr>
<ul>
<li>check if platform needs to be added and if yes, then add a platform of type Web with redirect URI as <code>http://localhost/red</code> and select <code>ID tokens (used for implicit and hybrid flows)</code></li>
</ul>
<hr>
<ul>
<li>
<p>If you want group to be part of your OIDC, under Token configuration click Add groups claim. Select Security groups and Group ID. Groups created in AAD can only be included by their ObjectID and not name.</p>
</li>
<li>
<p>Copy the <code>Application (client) ID</code> and <code>Directory (tenant) ID</code> to be used later.</p>
</li>
</ul>
<h3 id="configure-amazon-eks">Configure Amazon EKS</h3>
<p>Amazon provides a way to configure OIDC compatible identity provider via the management console. Navigate to Authentication under Configuration in the EKS cluster panel when you select your cluster.</p>
<ul>
<li>Click on <code>Associate Identity Provider</code>
<ul>
<li>Issuer URL: <code>https://sts.windows.net/[Directory (tenant) ID]</code></li>
<li>Client ID: <code>[Application (client) ID]</code></li>
<li>Username claim: <code>email</code>
<ul>
<li>(this will be <code>upn</code> if you want to authenticate direct member users of Azure AD and not guest users)</li>
</ul>
</li>
<li>Groups claim: <code>groups</code></li>
<li>Username prefix: <code>aad:</code></li>
<li>Groups prefix: <code>aad:</code></li>
</ul>
</li>
<li>If you want to add tags to identify the service principal or any other detail, you can add tags and save.</li>
<li>This will update your OIDC identity provider in the API server and this takes sometime.(For me it took almost 40-50 minutes on each trial)</li>
</ul>
<p>Now the server configuration is completed. We will proceed with configuring clients</p>
<h3 id="client-configuration">Client configuration</h3>
<p>Since mostly, we use <code>kubectl</code> to interact with Kubernetes, we will configure kubectl to use <a href="https://github.com/Azure/kubelogin">kubelogin</a> which is a <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#client-go-credential-plugins">client-go credential (exec) plugin</a> implementing azure authentication. This plugin provides features that are not available in kubectl. It is supported on kubectl v1.11+</p>
<h4 id="install-azurekubelogin">Install Azure/kubelogin</h4>
<p>I followed the installation instructions from <a href="https://github.com/Azure/kubelogin">https://github.com/Azure/kubelogin</a>:</p>
<p>Install using homebrew:</p>
<pre tabindex="0"><code>brew install Azure/kubelogin/kubelogin
</code></pre><p>Install directly from Github</p>
<pre tabindex="0"><code>wget https://github.com/Azure/kubelogin/releases/latest/download/kubelogin-linux-amd64.zip
unzip kubelogin-linux-amd64.zip -d kubelogin
mv kubelogin/bin/linux_amd64/kubelogin /usr/local/bin/
rm -r kubelogin*
</code></pre><h4 id="configure-kubectl">Configure kubectl</h4>
<p>Below kubeconfig contains sample garbage value, please replace the below fields with proper value</p>
<ul>
<li>certificate-authority-data</li>
<li>server</li>
<li>value for server-id</li>
<li>value for client-id</li>
<li>value for tenant-id</li>
</ul>
<pre tabindex="0"><code>apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tC ...REDACTED STRING
    server: https://34DA2A37GFSDXY7GFYWGE7ABA34Q11R0.gr7.us-east-1.eks.amazonaws.com
  name: aws-eks-aad
contexts:
- context:
    cluster: aws-eks-aad
    user: azure-user
  name: aws-eks-azure-user
current-context: aws-eks-azure-user
kind: Config
preferences: {}
users:
- name: azure-user
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - get-token
      - --environment
      - AzurePublicCloud
      - --server-id
      - a3xxxx4fe-xxxx-xxxx-xxxx-dexxxxxx210
      - --client-id
      - a3xxxx4fe-xxxx-xxxx-xxxx-dexxxxxx210
      - --tenant-id
      - 67xxx5c-xxxx-xxxx-xxxx-254xxxxx9ccf
      command: kubelogin
      env: null
</code></pre><h3 id="authentication">Authentication</h3>
<p>Post completion of this setup, issue <code>kubectl</code> command to get the instruction to authenticate yourself; Note this will only authenticate you, you would need to configure RBAC to allow the users to interact with cluster.</p>
<pre tabindex="0"><code>kubectl get pods
To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code EJQH9Q8LS to authenticate.
</code></pre><h3 id="authorization">Authorization</h3>
<p>We would need to setup RBAC for different users and group as per our need, for that I created 3 Azure AD groups and mapped then as below</p>
<ol>
<li>eks-admins</li>
<li>eks-editors</li>
<li>eks-viewers</li>
</ol>
<pre tabindex="0"><code>kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  annotations:
    for.ref.AAD.Group: https://portal.azure.com/#blade/Microsoft_AAD_IAM/GroupDetailsMenuBlade/Overview/groupId/abe95a71-b52a-43f2-9095-fecd4f6ef58d
  name: aad-cluster-admins
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: aad:abe95a71-b52a-43f2-9095-fecd4f6ef58d  # group ID setup
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: aad:rizwan.khan@mycontoso.com  # user setup
  
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    for.ref.AAD.Group: https://portal.azure.com/#blade/Microsoft_AAD_IAM/GroupDetailsMenuBlade/Overview/groupId/0bec3baa-511d-4816-ae62-2758d6023cf1
  name: aad-cluster-editors
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: edit
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: aad:0bec3baa-511d-4816-ae62-2758d6023cf1

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  annotations:
    for.ref.AAD.Group: https://portal.azure.com/#blade/Microsoft_AAD_IAM/GroupDetailsMenuBlade/Overview/groupId/03e95a71-b92a-4cf2-9895-fecd3f6ef58d
  name: aad-cluster-viewers
roleRef:
  kind: ClusterRole
  name: view
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: Group
  name: aad:03e95a71-b92a-4cf2-9895-fecd3f6ef58d
  apiGroup: rbac.authorization.k8s.io
</code></pre><p>After these RBAC are set, I added my user and couple of other is those group and voila, we were all set and good to go.</p>
]]></content></item><item><title>Trivy - Scan Container Images</title><link>https://rizwan-kh.github.io/posts/2021/12/trivy-scan-container-images/</link><pubDate>Tue, 14 Dec 2021 23:10:21 +0400</pubDate><guid>https://rizwan-kh.github.io/posts/2021/12/trivy-scan-container-images/</guid><description>Trivy Trivy is a scanner for vulnerabilities in container images, file systems, git repositories and configuration. It&amp;rsquo;s an Aqua Security open source project that can be easily used to integrate with our existing CI CD pipeline or used as an stand alone tool to scan container images deployed on Kubernetes cluster.
We were using Clair previous to our switch to Trivy as we analyzed both the tools and found Trivy to be more helpful for the longer run - it was faster, had more CVE database updates as compared to Clair and didn&amp;rsquo;t depend on external clients to interact/scan.</description><content type="html"><![CDATA[<p><img src="/trivy.png" alt="trivy"></p>
<h2 id="trivy">Trivy</h2>
<p><a href="https://github.com/aquasecurity/trivy">Trivy</a> is a scanner for vulnerabilities in container images, file systems, git repositories and configuration. It&rsquo;s an <a href="https://aquasec.com/">Aqua Security</a> open source project that can be easily used to integrate with our existing CI CD pipeline or used as an stand alone tool to scan container images deployed on Kubernetes cluster.</p>
<p>We were using <a href="https://github.com/coreos/clair">Clair</a> previous to our switch to Trivy as we analyzed both the tools and found Trivy to be more helpful for the longer run - it was faster, had more CVE database updates as compared to Clair and didn&rsquo;t depend on external clients to interact/scan.</p>
<h2 id="usage">Usage</h2>
<p>For stand-alone usage, simply <a href="https://github.com/aquasecurity/trivy/releases">download the binary</a> on your Mac/Linux system and scan images or file systems or configuration as below</p>
<pre tabindex="0"><code># for scaning images
trivy image [image-name]
trivy image alpine:latest

# for scanning file system
trivy fs --security-checks vuln,config [YOUR_PROJECT_DIR]
trivy fs --security-checks vuln,config my_project/

# for scanning configuration
trivy config [YOUR_IAC_DIR]
trivy config terraform/src
</code></pre><p>Trivy is a <a href="https://github.com/aquasecurity/trivy#features">feature rich tool</a> and once you start using the same, you will recommend to everyone who isn&rsquo;t using it due to its simplicity.</p>
<h2 id="use-case">Use-case</h2>
<p>We scan the images in our build CI pipeline, but once deployed on the Kubernetes cluster, we were trying to find a tool which would scan the deployed images, we found Trivy can be used as an alternative, if you don&rsquo;t have run-time scanners like Palo Alto Twistlock Defender which does more than just scanning.</p>
<p>We basically were re-scanning the images from our registry, but only the ones which are already running on our cluster.</p>
<p>We get the list of images using the below <code>kubectl</code> API call and then iterate over those images to generate either HTML or JSON format reports as per the need.</p>
<pre tabindex="0"><code># to get the list of images running on the cluster
kubectl get pods --all-namespaces -o jsonpath=&#34;{.items[*].spec.containers[*].image}&#34; |tr -s &#39;[[:space:]]&#39; &#39;\n&#39; |sort |uniq -c

# then we use the output list from above command to iterate over each image to get the desired output
# to get only critical vulnerability on library layer
trivy image -s CRITICAL --vuln-type library --ignore-unfixed --format json -o my-app-1.4.3.json registry.mycompany.com/containers/my-app:1.4.3

# to get all vulnerability in HTML format
trivy image --format template &#34;@contrib/template.html&#34; -o my-app-1.4.3.html registry.mycompany.com/containers/my-app:1.4.3
</code></pre>]]></content></item><item><title>Traefik on Kubernetes with Let's Encrypt &amp; Route53</title><link>https://rizwan-kh.github.io/posts/2021/07/traefik-on-kubernetes-with-lets-encrypt-route53/</link><pubDate>Wed, 28 Jul 2021 17:40:27 +0400</pubDate><guid>https://rizwan-kh.github.io/posts/2021/07/traefik-on-kubernetes-with-lets-encrypt-route53/</guid><description>Why Traefik? Traefik is a modern dynamic load balancer and reverse proxy, its easy to setup, control and provides lots of options which sits right with our use-cases. It integrates with Lets Encrypt to provide SSL termination along with support for service discovery, tracing, metrics out of the box running on Kubernetes as a small pod.
Since when We have been using Traefik since early 2017 on 2 of our cluster as a daemonset Kubernetes object with externalDNS to update the worker nodes IP with our DNS provider and used to manually generate and update Lets Encrypt certificate on a Kubernetes secret object.</description><content type="html"><![CDATA[<p><img src="/k8s-traefik-le-route53.svg" alt="k8s-traefik-le-route53"></p>
<h2 id="why-traefik">Why Traefik?</h2>
<p><a href="https://traefik.io/traefik/"><strong>Traefik</strong></a> is a modern dynamic load balancer and reverse proxy, its easy to setup, control and provides lots of options which sits right with our use-cases. It integrates with Lets Encrypt to provide SSL termination along with support for service discovery, tracing, metrics out of the box running on Kubernetes as a small pod.</p>
<h3 id="since-when">Since when</h3>
<p>We have been using Traefik since early 2017 on 2 of our cluster as a daemonset Kubernetes object with externalDNS to update the worker nodes IP with our DNS provider and used to manually generate and update Lets Encrypt certificate on a Kubernetes secret object.</p>
<p>Later, with the release of Traefik proxy v2, we started deploying Traefik as a deployment Kubernetes object and have been using it since then on more than 10 clusters with auto Lets Encrypt certificate generation and we use a small custom init-container to update the Route53 DNS entry.</p>
<h2 id="how-is-the-entire-setup-done">How is the entire setup done</h2>
<p>It might feel a bit sketchy if you&rsquo;re doing this for the first time, but once setup - you&rsquo;re good to go for almost till you don&rsquo;t want to upgrade. üòâ</p>
<h3 id="pre-requisite">Pre-requisite</h3>
<ul>
<li>AWS IAM Keys with Route53 list and update access</li>
<li>AWS Route53 Public Hosted Zone</li>
<li>A working Kubernetes cluster</li>
</ul>
<h4 id="first-thing-first">First thing first</h4>
<p>Setup your Kubernetes cluster, doens&rsquo;t matter if it&rsquo;s EKS, AKS, GKE, Custom kubeadm, RKE, K3S, KOPS, etc. Once the cluster is setup and ready; we would deploy the below kubernetes manifest yaml (Now, Traefik support deployment via Helm), we use kustomize to deploy Traefik on each of our cluster from our GitLab CI CD pipeline (will not deep dive on that here, just will show you the manifest to start with)</p>
<h4 id="lets-start-with-traefik-deployment">Lets Start with Traefik Deployment</h4>
<p>Deploy the below traefik-crd-sa-cr-crb.yaml file with <code>kubectl apply -f traefik-crd-sa-cr-crb.yaml</code> as-is.</p>
<p><a href="https://doc.traefik.io/traefik/user-guides/crd-acme/#cluster-resources">The below CRD&rsquo;s and supporting manifest is taken from this Traefik official documentation</a></p>
<pre tabindex="0"><code># traefik-crd-sa-cr-crb.yaml
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: ingressroutes.traefik.containo.us

spec:
  group: traefik.containo.us
  version: v1alpha1
  names:
    kind: IngressRoute
    plural: ingressroutes
    singular: ingressroute
  scope: Namespaced

---
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: middlewares.traefik.containo.us

spec:
  group: traefik.containo.us
  version: v1alpha1
  names:
    kind: Middleware
    plural: middlewares
    singular: middleware
  scope: Namespaced

---
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: ingressroutetcps.traefik.containo.us

spec:
  group: traefik.containo.us
  version: v1alpha1
  names:
    kind: IngressRouteTCP
    plural: ingressroutetcps
    singular: ingressroutetcp
  scope: Namespaced

---
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: ingressrouteudps.traefik.containo.us

spec:
  group: traefik.containo.us
  version: v1alpha1
  names:
    kind: IngressRouteUDP
    plural: ingressrouteudps
    singular: ingressrouteudp
  scope: Namespaced

---
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: tlsoptions.traefik.containo.us

spec:
  group: traefik.containo.us
  version: v1alpha1
  names:
    kind: TLSOption
    plural: tlsoptions
    singular: tlsoption
  scope: Namespaced

---
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: tlsstores.traefik.containo.us

spec:
  group: traefik.containo.us
  version: v1alpha1
  names:
    kind: TLSStore
    plural: tlsstores
    singular: tlsstore
  scope: Namespaced

---
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: traefikservices.traefik.containo.us

spec:
  group: traefik.containo.us
  version: v1alpha1
  names:
    kind: TraefikService
    plural: traefikservices
    singular: traefikservice
  scope: Namespaced

---
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: serverstransports.traefik.containo.us

spec:
  group: traefik.containo.us
  version: v1alpha1
  names:
    kind: ServersTransport
    plural: serverstransports
    singular: serverstransport
  scope: Namespaced

---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: traefik-ingress-controller

rules:
  - apiGroups:
      - &#34;&#34;
    resources:
      - services
      - endpoints
      - secrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - extensions
      - networking.k8s.io
    resources:
      - ingresses
      - ingressclasses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - extensions
    resources:
      - ingresses/status
    verbs:
      - update
  - apiGroups:
      - traefik.containo.us
    resources:
      - middlewares
      - ingressroutes
      - traefikservices
      - ingressroutetcps
      - ingressrouteudps
      - tlsoptions
      - tlsstores
      - serverstransports
    verbs:
      - get
      - list
      - watch

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: traefik-ingress-controller

roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: traefik-ingress-controller
subjects:
  - kind: ServiceAccount
    name: traefik-ingress-controller
    namespace: default
---
apiVersion: v1
kind: ServiceAccount
metadata:
  namespace: default
  name: traefik-ingress-controller
</code></pre><p>Now save the below manifest and deploy it on your cluster with few amendments like replacing proper values for AWS Route53 hosted zone and IAM keys, storageClassName, acme email address, your sub-domain in the initContainer args section</p>
<p>‚ÑπÔ∏è - you can use Instance profile as well if you don&rsquo;t want to use IAM keys</p>
<pre tabindex="0"><code># create pvc to store the Lets Encrypt cert to persist between Traefik restarts
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
  name: traefik-acme-storage-default
  namespace: default
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: gp2
---
# replace the below values for data with the proper encoded value (these are random garbage values üòù ) 
apiVersion: v1
data:
  access_key_id: S1FSMjI0NDRKNVNBSUtBRDRLTkgK  
  hosted_zone: WjM0RzY1QUVERURFCg==
  region: dXMtZWFzdC0x
  secret_key_id: SmszMmRUUUJWRlZTVVlCWVZESVVJRVZWSTZHVkhHMzJRVwo=
kind: Secret
metadata:
  labels:
  name: route53-secret
  namespace: default
type: Opaque
---
# create Traefik clusterIP service, note we won&#39;t be using the web as we avoid using port 80 for all Production purposes
apiVersion: v1
kind: Service
metadata:
  labels:
  name: traefik
  namespace: default
spec:
  ports:
  - name: web
    port: 8000
    protocol: TCP
    targetPort: 8000
  - name: admin
    port: 8080
    protocol: TCP
    targetPort: 8080
  - name: websecure
    port: 4443
    protocol: TCP
    targetPort: 4443
  selector:
    app: traefik
  type: ClusterIP
---
# Traefik deployment yaml, initContainers will be explained in detail below
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: traefik
  name: traefik
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: traefik
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: traefik
    spec:
      containers:
      - args:
        - --api.insecure
        - --accesslog=true
        - --entrypoints.websecure.Address=:4443
        - --providers.kubernetescrd
        - --providers.kubernetesingress
        - --entrypoints.websecure.http.tls.options=default
        - --entrypoints.websecure.http.tls.certResolver=default
        - --certificatesresolvers.default.acme.dnschallenge.provider=route53
        - --certificatesResolvers.default.acme.dnsChallenge.delayBeforeCheck=5
        - --certificatesresolvers.default.acme.email=my.email@xzy.com
        - --certificatesresolvers.default.acme.storage=/data/acme.json
        - --metrics.prometheus=true
        - --metrics.prometheus.entryPoint=metrics
        - --serverstransport.insecureskipverify
        env:
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              key: access_key_id
              name: route53-secret
        - name: AWS_HOSTED_ZONE_ID
          valueFrom:
            secretKeyRef:
              key: hosted_zone
              name: route53-secret
        - name: AWS_REGION
          valueFrom:
            secretKeyRef:
              key: region
              name: route53-secret
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              key: secret_key_id
              name: route53-secret
        image: traefik:v2.4
        imagePullPolicy: IfNotPresent
        name: traefik
        ports:
        - containerPort: 4443
          hostPort: 443
          name: websecure
          protocol: TCP
        - containerPort: 8080
          name: admin
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /data
          name: cert-storage-volume
      dnsPolicy: ClusterFirst
      initContainers:
      - args:
        - &#39;apt-get install awscli -y;HOSTED_ZONE_ID=&#34;Z34G65AEDEDE&#34;;INPUT=&#34;{\&#34;ChangeBatch\&#34;: { \&#34;Comment\&#34;:
          \&#34;Update the A record set\&#34;, \&#34;Changes\&#34;:[ {\&#34;Action\&#34;: \&#34;UPSERT\&#34;, \&#34;ResourceRecordSet\&#34;:
          { \&#34;Name\&#34;: \&#34;*.k8s.mydomain.com\&#34;, \&#34;Type\&#34;: \&#34;A\&#34;, \&#34;TTL\&#34;:
          300, \&#34;ResourceRecords\&#34;: [ { \&#34;Value\&#34;: \&#34;HOSTIP\&#34;}]}}]}}&#34;; HOSTIP=`curl
          http://169.254.169.254/latest/meta-data/local-ipv4`;INPUT_JSON=`echo $INPUT
          | sed &#34;s/HOSTIP/$HOSTIP/&#34;`; echo $INPUT_JSON;aws route53 change-resource-record-sets
          --hosted-zone-id &#34;$HOSTED_ZONE_ID&#34; --cli-input-json &#34;$INPUT_JSON&#34;&#39;
        command:
        - /bin/sh
        - -c
        env:
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              key: access_key_id
              name: route53-secret
        - name: AWS_REGION
          valueFrom:
            secretKeyRef:
              key: region
              name: route53-secret
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              key: secret_key_id
              name: route53-secret
        envFrom:
        - secretRef:
            name: route53-secret
        image: ubuntu:xenial    
        imagePullPolicy: IfNotPresent
        name: route53-changes
      serviceAccount: traefik-ingress-controller
      serviceAccountName: traefik-ingress-controller
      volumes:
      - name: cert-storage-volume
        persistentVolumeClaim:
          claimName: traefik-acme-storage-default
----
</code></pre><p>The manifest is pretty straight forward, with the only non-standard part being the initContainer, which is used here to get the IP address of the worker node on which Trafik is deployed and to update it in AWS Route53 with an A record.</p>
<p>Assuming the IP address is 10.7.60.70, an entry with record *.k8s.mydomain.com with an A record pointing to IP 10.7.60.70 will get inserted in Route53 in your public hosted zone.</p>
<h4 id="is-this-even-working">Is this even working?</h4>
<p>Ofcourse, if we have done all this, how do we validate, for that we will deploy a service and ingressRoute CRD with the manifest provided below</p>
<p><code>kubectl apply -f traefik-ir.yaml</code></p>
<pre tabindex="0"><code># traefik-ir.yaml
apiVersion: traefik.containo.us/v1alpha1
kind: IngressRoute
metadata:
  name: traefik-dashboard
  namespace: default
spec:
  entryPoints:
    - websecure
  routes:
  - match: Host(`traefik.k8s.mydomain.com`) &amp;&amp; PathPrefix(`/`)
    kind: Rule
    services:
    - name: admin
      port: 8080
  tls:
    certResolver: default
</code></pre><p>You should now be able to browse to this URL <a href="https://traefik.k8s.mydomain.com">https://traefik.k8s.mydomain.com</a> with proper Lets Encrypt TLS certificates and would see the Treafik dashboard.</p>
<p><img src="/traefik-dashboard.webp" alt="traefik-dashboard"></p>
<h3 id="final-thoughts">Final thoughts</h3>
<p>This blog may not do justice in explaining how this tools exactly works, but my intent was to get you a working traefik deployment with AWS Route53, and I guess that would help someone in need.</p>
]]></content></item><item><title>(Azure DevOps) Send Json Request(Parameters) to Azure Pipelines</title><link>https://rizwan-kh.github.io/posts/2021/06/azure-devops-send-json-requestparameters-to-azure-pipelines/</link><pubDate>Mon, 21 Jun 2021 11:16:23 +0400</pubDate><guid>https://rizwan-kh.github.io/posts/2021/06/azure-devops-send-json-requestparameters-to-azure-pipelines/</guid><description>Introduction If you&amp;rsquo;ve been using Azure DevOps, you would know that a pipeline can be trigger with runtime parameters in the format key: value pair and this is great for doing almost all of the tasks.
For our use-case, we had been looking at option to send a JSON based parameter dictionary and I couldn&amp;rsquo;t find any way at the time of writing this article. We came up with a hack to achieve this and I would want to write it up in this blog post.</description><content type="html"><![CDATA[<p><img src="/azure-pipelines.png" alt="azure"></p>
<h3 id="introduction">Introduction</h3>
<p>If you&rsquo;ve been using Azure DevOps, you would know that a pipeline can be trigger with runtime parameters in the format <code>key: value</code> pair and this is great for doing almost all of the tasks.</p>
<p>For our use-case, we had been looking at option to send a JSON based parameter dictionary and I couldn&rsquo;t find any way at the time of writing this article. We came up with a hack to achieve this and I would want to write it up in this blog post.</p>
<h3 id="flatten-json--operation">Flatten JSON &amp; operation</h3>
<p>Wee look at the option to use the Flatten JSON Objects extension to convert a nested data layer object into a new object with only one layer of key/value pairs. For this we used the <strong>flatten_json</strong> library</p>
<p><code>pip install flatten_json</code></p>
<pre tabindex="0"><code>&#39;&#39;&#39;
flatten the json data with a &#39;_&#39; separator; you can use different separator as well
&#39;&#39;&#39;
# flatten.py
import flatten_json

data = {
    &#34;a&#34;: 1,
    &#34;b&#34;: 2,
    &#34;c&#34;: [{&#34;d&#34;: [2, 3, 4], &#34;e&#34;: [{&#34;f&#34;: 1, &#34;g&#34;: 2}]}]
}
flat_json = flatten_json.flatten(data,&#39;_&#39;)
print(flat_json)
&#39;&#39;&#39;
the above command gives us below output, which is single layer JSON
{
 &#39;a&#39;: 1,
 &#39;b&#39;: 2,
 &#39;c_0_d_0&#39;: 2,
 &#39;c_0_d_1&#39;: 3,
 &#39;c_0_d_2&#39;: 4,
 &#39;c_0_e_0_f&#39;: 1,
 &#39;c_0_e_0_g&#39;: 2
 }
&#39;&#39;&#39;
</code></pre><p>Now that we had single layer <code>key: value</code> pair, we added another hack to be able to read these variables properly in Azure DevOps pipeline. We added a prefix (any prefix that&rsquo;s not part of your JSON) <strong>RizwanGotNoChill-</strong></p>
<pre tabindex="0"><code>prefixed_flat_json = {f&#34;RizwanGotNoChill-{key}&#34;: val for key, val in flat_json.items()}
print(prefixed_flat_json)
&#39;&#39;&#39;
the above command gives us below output, which is single layer prefixed JSON
{
 &#39;RizwanGotNoChill-a&#39;: 1,
 &#39;RizwanGotNoChill-b&#39;: 2,
 &#39;RizwanGotNoChill-c_0_d_0&#39;: 2,
 &#39;RizwanGotNoChill-c_0_d_1&#39;: 3,
 &#39;RizwanGotNoChill-c_0_d_2&#39;: 4,
 &#39;RizwanGotNoChill-c_0_e_0_f&#39;: 1,
 &#39;RizwanGotNoChill-c_0_e_0_g&#39;: 2
 }
&#39;&#39;&#39;
</code></pre><p>This part if all we needed, now you could use the Azure DevOps Rest API with above <code>key: value</code> pair to trigger any pipeline and I will show, how we intepreted these at the pipeline and crafted a JSON out.</p>
<h3 id="trigger-azure-devops-pipeline">Trigger Azure DevOps Pipeline</h3>
<p>I found the below two ways to trigger a pipeline, you may want to see which one fits your case -</p>
<ul>
<li>Using REST API based curl/http command calling the <strong>build API</strong> - <a href="https://dev.azure.com/YOURORG/YOURPROJECT/_apis/build/builds?api-version=6.1-preview.6">https://dev.azure.com/YOURORG/YOURPROJECT/_apis/build/builds?api-version=6.1-preview.6</a>)</li>
</ul>
<ol>
<li>Convert you JSON to an escaped one using this <a href="https://jsonformatter.org/json-escape">link</a></li>
<li>Create the request param as below and send</li>
</ol>
<pre tabindex="0"><code># Replace the values for YOURORG, YOURPROJECT, PATTOKEN and the value for id(pipeline ID)
curl --request POST \
  --url &#39;https://dev.azure.com/YOURORG/YOURPROJECT/_apis/build/builds?api-version=6.1-preview.6&#39; \
  --header &#39;Authorization: Basic PATTOKEN&#39; \
  --header &#39;Content-Type: application/json&#39; \

  --data &#39;{
    &#34;parameters&#34;: &#34;{\&#34;RizwanGotNoChill-a\&#34;:1,\&#34;RizwanGotNoChill-b\&#34;:2,\&#34;RizwanGotNoChill-c_0_d_0\&#34;:2,\&#34;RizwanGotNoChill-c_0_d_1\&#34;:3,\&#34;RizwanGotNoChill-c_0_d_2\&#34;:4,\&#34;RizwanGotNoChill-c_0_e_0_f\&#34;:1,\&#34;RizwanGotNoChill-c_0_e_0_g\&#34;:2}&#34;,
    &#34;definition&#34;:  { &#34;id&#34;:  27 }
}&#39;
</code></pre><ul>
<li>The other way to trigger a pipeline is calling the <strong>RunPipeline API</strong>, I wrote a small program</li>
</ul>
<script type="application/javascript" src="https://gist.github.com/rizwan-kh/c08955a24bfc1eb3eaa48248acd012e0.js"></script>

<h3 id="unflatten-json--operation">Unflatten JSON &amp; operation</h3>
<p>Now the baton is at the Azure DevOps end to decode and re-configure this as a JSON, I used a simple job and Python script to achieve this</p>
<pre tabindex="0"><code># azure-pipeline.yml
- job: install_library
  steps:
  - checkout: none
  - script: |
      pip3 install flatten_json
      pip3 freeze
    displayName: Install python library

- job: generate_files
  steps:
  - script: |
      echo &#34;Here, we will grep the content based on the prefix and store in a file&#34;
      env | grep RizwanGotNoChill &gt; request.txt
      sed -i &#39;s/RizwanGotNoChill-//&#39; request.txt
      echo &#34;utils.py first creates a dict based on request.txt and then unflattens the files and write to request.json&#34;
      python3 utils.py
      cat request.json | jq .
    displayName: Produce JSON
</code></pre><pre tabindex="0"><code># utils.py
import os, sys
import json
import flatten_json

dict_from_file = {}
with open(&#34;request.txt&#34;) as fp:
    for line in fp:
        (key, val) = line.split(&#34;=&#34;)
        dict_from_file[key.lower()] = val.rstrip(&#34;\n&#34;)
print(dict_from_file)

request_json = flatten_json.unflatten(dict_from_file, &#39;-&#39;)
print(request_json)

with open(&#39;request.json&#39;, &#39;w&#39;) as fp:
     fp.write(json.dumps(request_json))
</code></pre><p>Here our <code>request.json</code> is a proper JSON file ready to be used by any application/program in our Azure Pipeline;
We noticed one issue though, the conversion process, converts all other data types to string data-type, so might be you would need to change that or add some logic to create json/dict with type safe.</p>
]]></content></item><item><title>Hugo &amp; GitHub - Setup Static Website on GitHub</title><link>https://rizwan-kh.github.io/posts/2020/06/hugo-github-setup-static-website-on-github/</link><pubDate>Fri, 12 Jun 2020 22:32:01 +0400</pubDate><guid>https://rizwan-kh.github.io/posts/2020/06/hugo-github-setup-static-website-on-github/</guid><description>Hugo and Github Account A small write-up on how this site was setup. There are plenty of materials available all over the internet, but this is more of a self-help document for myself to refer back in future to understand how this was all setup.
Hugo can be downloaded from their official release page A GitHub account needs to be created from this link and a public repo is required to be setup here After installing Hugo, to generate a new site, run the below command:</description><content type="html"><![CDATA[<p><img src="/hugo-logo-wide.svg" alt="Hub"></p>
<h2 id="hugo-and-github-account">Hugo and Github Account</h2>
<p>A small write-up on how <a href="https://rizwan-kh.github.io">this site</a> was setup. There are plenty of materials available all over the internet, but this is more of a self-help document for myself to refer back in future to understand how this was all setup.</p>
<ul>
<li>Hugo can be downloaded from their official <a href="https://github.com/gohugoio/hugo/releases">release page</a></li>
<li>A GitHub account needs to be created from <a href="https://github.com/join">this link</a> and a public repo is required to be setup <a href="https://github.com/new">here</a></li>
</ul>
<p>After installing <strong>Hugo</strong>, to generate a new site, run the below command:</p>
<pre tabindex="0"><code>hugo new site rizwan-blogs
</code></pre><p>This will automatically create a directory named <em>rizwan-blogs</em> which will contain the necessary sub-directories and files required for our new hugo-based static website.</p>
<p>The <code>config.toml</code> file is the main configuration file where we can add/change the theme or add additional settings like title, author, etc. Sample <strong>config.toml</strong> looks like below</p>
<pre tabindex="0"><code>baseURL = &#34;https://rizwan-kh.github.io&#34;
title = &#34;Rizwan Khanüë®üèª‚Äçüíª&#34;
languageCode = &#34;en-us&#34;
theme = &#34;hello-friend-ng&#34;
</code></pre><p>Replace the above 4 values as you want to in the config.toml and you are good to go.</p>
<p>To add content/pages, we run the below command, this will create a markdown page with the post marked as <code>draft: true</code> and we can keep it as true until we want to publish the post.</p>
<pre tabindex="0"><code>huge new posts/my-post.md
</code></pre><p>Fire up your favorite editor and it&rsquo;s all <a href="https://www.markdownguide.org/cheat-sheet/">markdown</a> from there.
Then we can start the hugo server in build drafts enabled mode (or as I think of it as development mode) with the below command and it will let you browse the site on address <a href="http://localhost:1313/">localhost:1313</a> with auto-refresh mode on.</p>
<pre tabindex="0"><code>hugo server -D
</code></pre><p>When the pages/posts are ready to be published, simply run the server in publish mode and hugo will take care of generating the required files in <code>./public</code> directory,
<em>Note: Make sure to clean up the public directory, as the directory will contain drafts pages as well from the previous run</em></p>
<pre tabindex="0"><code>hugo
</code></pre><h4 id="deploy-your-website">Deploy your website</h4>
<p>The final production content in stored in the public directory and we just need to copy the directory in the production server either on <strong>AWS S3</strong>, or on <strong>GitHub Pages</strong></p>
<hr>
<p>Now for the publishing the website, Go to <a href="https://github.com">GitHub</a> and create a <a href="https://github.com/new">new public repository</a> named <em><strong>username.github.io</strong></em>, where <em><strong>username</strong></em> is your username (or organization name) on GitHub.</p>
<p>Use any <strong>git</strong> client to clone the repo, copy over the entire <em>public</em> directory</p>
<pre tabindex="0"><code>git clone https://github.com/username/username.github.io
cd username.github.io
</code></pre><p>copy over the entire content of the <code>public</code> directory <code>username.github.io</code> and use git to commit and push the content over to <em>GitHub</em></p>
<pre tabindex="0"><code>git add --all
git commit -m &#34;initial commit&#34;
git push origin main
</code></pre><p>open your browser and type up the URL <a href="https://username.github.io">username.github.io</a> and voila your static website content is up and ready for free.</p>
]]></content></item><item><title>Klar</title><link>https://rizwan-kh.github.io/posts/2020/03/klar/</link><pubDate>Sat, 14 Mar 2020 17:20:26 +0400</pubDate><guid>https://rizwan-kh.github.io/posts/2020/03/klar/</guid><description>Introduction Klar is a static binary tool to analyze images stored in a private or public Docker registry for security vulnerabilities using Clair. Klar is designed to be used as an integration tool so it relies on environment variables. It&amp;rsquo;s a single binary which requires no dependencies and can be plugged and/or integrated into our CI CD pipelines.
Klar serves as a client which coordinates the image checks between the Docker registry and Clair.</description><content type="html"><![CDATA[<h3 id="introduction">Introduction</h3>
<p><a href="https://github.com/optiopay/klar">Klar</a> is a static binary tool to analyze images stored in a private or public Docker registry for security vulnerabilities using <a href="https://github.com/coreos/clair">Clair</a>. Klar is designed to be used as an integration tool so it relies on environment variables. It&rsquo;s a single binary which requires no dependencies and can be plugged and/or integrated into our CI CD pipelines.</p>
<p>Klar serves as a client which coordinates the image checks between the Docker registry and Clair. We heavily use klar along with Clair in our internal CI CD pipeline to scan the newly built Docker images.</p>
<hr>
<h4 id="installation">Installation</h4>
<p>Download the latest release (for OSX and Linux) from <a href="https://github.com/optiopay/klar/releases/">https://github.com/optiopay/klar/releases/</a> and put the binary in a directory which is present in your PATH variable (make sure it has execute permission set).</p>
<hr>
<h4 id="usage">Usage</h4>
<p>Klar process returns if 0 if the number of detected high severity vulnerabilities in an image is less than or equal to a threshold (see below) and 1 if there were more. It will return 2 if an error has prevented the image from being analyzed.</p>
<p>Klar can be configured via the following environment variables:</p>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>CLAIR_ADDR</strong></td>
<td>address of Clair server. It has a form of protocol://host:port - protocol and port default to http and 6060 respectively and may be omitted. You can also specify basic authentication in the URL: protocol://login:password@host:port</td>
</tr>
<tr>
<td><strong>CLAIR_OUTPUT</strong></td>
<td>severity level threshold, vulnerabilities with severity level higher than or equal to this threshold will be outputted. Supported levels are Unknown, Negligible, Low, Medium, High, Critical, Defcon1. Default is Unknown</td>
</tr>
<tr>
<td><strong>CLAIR_THRESHOLD</strong></td>
<td>how many outputted vulnerabilities Klar can tolerate before returning 1. Default is 0</td>
</tr>
<tr>
<td><strong>CLAIR_TIMEOUT</strong></td>
<td>timeout in minutes before Klar cancels the image scanning. Default is 1</td>
</tr>
<tr>
<td><strong>DOCKER_USER</strong></td>
<td>Docker registry account name</td>
</tr>
<tr>
<td><strong>DOCKER_PASSWORD</strong></td>
<td>Docker registry account password</td>
</tr>
<tr>
<td><strong>DOCKER_TOKEN</strong></td>
<td>Docker registry account token. (Can be used in place of DOCKER_USER and DOCKER_PASSWORD)</td>
</tr>
<tr>
<td><strong>DOCKER_INSECURE</strong></td>
<td>Allow Klar to access registries with bad SSL certificates. Default is false. Clair will need to be booted with -insecure-tls for this to work</td>
</tr>
<tr>
<td><strong>DOCKER_TIMEOUT</strong></td>
<td>timeout in minutes when trying to fetch layers from a docker registry</td>
</tr>
<tr>
<td><strong>DOCKER_PLATFORM_OS</strong></td>
<td>The operating system of the Docker image. Default is linux. This only needs to be set if the image specified references a Docker ManifestList instead of a usual manifest</td>
</tr>
<tr>
<td><strong>DOCKER_PLATFORM_ARCH</strong></td>
<td>The architecture the Docker image is optimized for. Default is amd64. This only needs to be set if the image specified references a Docker ManifestList instead of a usual manifest</td>
</tr>
<tr>
<td><strong>REGISTRY_INSECURE</strong></td>
<td>Allow Klar to access insecure registries (HTTP only). Default is false</td>
</tr>
<tr>
<td><strong>JSON_OUTPUT</strong></td>
<td>Output JSON, not plain text. Default is false</td>
</tr>
<tr>
<td><strong>FORMAT_OUTPUT</strong></td>
<td>Output format of the vulnerabilities. Supported formats are standard, json, table. Default is standard. If JSON_OUTPUT is set to true, this option is ignored</td>
</tr>
<tr>
<td><strong>WHITELIST_FILE</strong></td>
<td>Path to the YAML file with the CVE whitelist. Look at whitelist-example.yaml for the file format</td>
</tr>
<tr>
<td><strong>IGNORE_UNFIXED</strong></td>
<td>Do not count vulnerabilities without a fix towards the threshold</td>
</tr>
</tbody>
</table>
<h4 id="usage-1">Usage:</h4>
<pre tabindex="0"><code>CLAIR_ADDR=localhost;CLAIR_OUTPUT=High;CLAIR_THRESHOLD=10;DOCKER_USER=docker;DOCKER_PASSWORD=secret;
klar mysql:latest
</code></pre><h5 id="debug-output">Debug Output</h5>
<p>You can enable more verbose output but setting KLAR_TRACE to true.</p>
<h2 id="run-export-klar_tracetrue-to-persist-between-runs">run <code>export KLAR_TRACE=true</code> to persist between runs.</h2>
<h4 id="gitlab-ci-usage">GitLab CI Usage</h4>
<p>We have the below job defined in <code>.gitlab-ci.yml</code> post Dockerbuild stage</p>
<pre tabindex="0"><code>image_analysis:
  stage: analyse
  image: registry.mycompany.com/ci/kubernetes-deploy:klar
  script:
    - export PATH=$PATH:$CI_PROJECT_DIR
    - export TAG=&#34;$CI_BUILD_REF_NAME&#34;
    - CLAIR_THRESHOLD=1000 DOCKER_TIMEOUT=5 CLAIR_TIMEOUT=5 CLAIR_ADDR=https://ngclair.mycompany.com:443 DOCKER_USER=gitlab-ci-token DOCKER_PASSWORD=$CI_BUILD_TOKEN klar $CI_REGISTRY_IMAGE:$TAG | tee scan.txt
  artifacts:
    paths:
      - $CI_PROJECT_DIR/scan.txt
    expire_in: 1 hour
  tags:
    - my_runner
</code></pre><p>This image <code>registry.mycompany.com/ci/kubernetes-deploy:klar</code> contains the klar binary and the clair server is running on <a href="https://ngclair.mycompany.com:443">https://ngclair.mycompany.com:443</a></p>
]]></content></item><item><title>Vagrant</title><link>https://rizwan-kh.github.io/posts/2020/03/vagrant/</link><pubDate>Sat, 14 Mar 2020 17:20:26 +0400</pubDate><guid>https://rizwan-kh.github.io/posts/2020/03/vagrant/</guid><description>Introduction Getting started with Vagrant Download the install the Vagrant from the Download section
vagrant version Installed Version: 2.2.10 Latest Version: 2.2.10 You&amp;#39;re running an up-to-date version of Vagrant! Download and install VirtualBox
Now if both Vagrant and VirtualBox are up and running, all we need to do is run the below 3 commands to have a Ubuntu VM up and available for our use in no time.
vagrant init hashicorp/bionic64 vagrant up vagrant ssh This is a pretty simple way to spin up a new Ubuntu VM, we can do a lot more which I will try to write up in later posts.</description><content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<hr>
<h2 id="getting-started-with-vagrant">Getting started with Vagrant</h2>
<p>Download the install the <a href="https://www.vagrantup.com/downloads">Vagrant</a> from the Download section</p>
<pre tabindex="0"><code>vagrant version
Installed Version: 2.2.10
Latest Version: 2.2.10

You&#39;re running an up-to-date version of Vagrant!
</code></pre><p>Download and install <a href="https://www.virtualbox.org/wiki/Downloads">VirtualBox</a></p>
<p>Now if both <strong>Vagrant</strong> and <strong>VirtualBox</strong> are up and running, all we need to do is run the below 3 commands to have a Ubuntu VM up and available for our use in no time.</p>
<pre tabindex="0"><code>vagrant init hashicorp/bionic64
vagrant up
vagrant ssh
</code></pre><p>This is a pretty simple way to spin up a new Ubuntu VM, we can do a lot more which I will try to write up in later posts.</p>
]]></content></item><item><title>Gitlab CI &amp; Kaniko to build Docker Images</title><link>https://rizwan-kh.github.io/posts/2020/02/gitlab-ci-kaniko-to-build-docker-images/</link><pubDate>Wed, 12 Feb 2020 12:11:00 +0400</pubDate><guid>https://rizwan-kh.github.io/posts/2020/02/gitlab-ci-kaniko-to-build-docker-images/</guid><description>Introduction You can build container images from a Dockerfile inside a container or a Kubernetes cluster, though J√©r√¥me Petazzoni strongly discourages from doing so. He wrote a detailed blog that can be read here on why not to build container images using Dockerfile inside a container or a Kubernetes cluster.
Context You will get N number of blogs on how to use the CI/CD of GitLab; here we will see an easy reference point to extend a file and create CI/CD for Docker Image to be built and stored in the same GitLab registry using kaniko.</description><content type="html"><![CDATA[<p><img src="/gitlab.jpeg" alt="GitLab"></p>
<h3 id="introduction">Introduction</h3>
<p>You can build container images from a Dockerfile inside a container or a Kubernetes cluster, though <em>J√©r√¥me Petazzoni</em> strongly discourages from doing so. He wrote a detailed blog that can be read <a href="http://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/">here</a> on why not to build container images using Dockerfile inside a container or a Kubernetes cluster.</p>
<hr>
<h4 id="context">Context</h4>
<p>You will get <code>N</code> number of blogs on how to use the CI/CD of GitLab; here we will see an easy reference point to extend a file and create CI/CD for Docker Image to be built and stored in the same GitLab registry using <a href="https://cloud.google.com/blog/products/gcp/introducing-kaniko-build-container-images-in-kubernetes-and-google-container-builder-even-without-root-access">kaniko</a>. This file needs to be created in the individual project in the GitLab using the template available with the name .gitlab-ci.yml.</p>
<hr>
<h4 id="what-is-kaniko">What is Kaniko?</h4>
<p><code>Note: Kaniko is not an officially supported Google product</code>
It is a tool to build container images from a Dockerfile inside a container or a Kubernetes cluster. It doesn&rsquo;t depend on the Docker daemon to run each Dockerfile command.</p>
<p>It comes with it&rsquo;s own limitations, but we don&rsquo;t run the risk of using Docker-in-Docker</p>
<hr>
<h4 id="prerequisites">Prerequisites</h4>
<ul>
<li>Access to GitLab (either private self hosted or managed)</li>
<li>GitLab project with a Dockerfile</li>
</ul>
<hr>
<h4 id="ci-yaml-for-auto-devops">CI YAML for auto-devops</h4>
<pre tabindex="0"><code># .gitlab-ci.yml
variables:
    GIT_SSL_NO_VERIFY: &#34;true&#34;

before_script:
  - echo &#34;Random image creation, user = $GITLABUSER&#34;

stages:
  - build

build_image:
  image:
    name: gcr.io/kaniko-project/executor:debug
    entrypoint: [&#34;&#34;]
  stage: build
  script:
    - ls
    - pwd
    - export CI_REGISTRY_IMAGE=mygitlab.com/base-project/subproject/project
    - echo &#34;{\&#34;auths\&#34;:{\&#34;mygitlab.com\&#34;:{\&#34;username\&#34;:\&#34;gitlab-ci-token\&#34;,\&#34;password\&#34;:\&#34;$CI_BUILD_TOKEN\&#34;},\&#34;repository.xyz-company.io\&#34;:{\&#34;username\&#34;:\&#34;user\&#34;,\&#34;password\&#34;:\&#34;123random\&#34;}}}&#34; &gt; /kaniko/.docker/config.json
    - wget https://letsencrypt.org/certs/lets-encrypt-x3-cross-signed.pem | xargs cat lets-encrypt-x3-cross-signed.pem &gt;&gt; /kaniko/ssl/certs/ca-certificates.crt
    - /kaniko/executor --skip-tls-verify --context $CI_PROJECT_DIR --dockerfile $CI_PROJECT_DIR/Dockerfile --destination $CI_REGISTRY_IMAGE:$CI_BUILD_REF_NAME
</code></pre><hr>
<p><strong>variables</strong>: These are static values which aren&rsquo;t going to change and is used at multiple location in the gitlab-ci.yml file</p>
<p><strong>before_script</strong>: Set(s) of commands or echo statement we want to print</p>
<p><strong>stages</strong>: Stages are block of code for an identical job or set of jobs viz. build, test, clean-up, delete, deploy, etc. This executes in the order it&rsquo;s defined in the YAML. A dot(.) in front of any job(block of code) disables it and it won&rsquo;t be executed or available neither as an automatic or manual job.</p>
<p><strong>Jobs (Each block of code)</strong>: Each block of individual stage contains key-value pair or set of commands to it. We can define each block of code to point to a particular stage and all the set of commands it requires to perform that function in the script block. It can be made to run automatically and also manual (start the job manually by clicking a button). The variables like password, access/secret key can be defined in the CI/CD settings under secret variables section so it&rsquo;s not available in plain text format.</p>
<hr>
<p><strong>Note</strong>: If you want to use the GitLab docker registry and store docker images in the GitLab project; this by default is disabled and needs to be enabled in the General setting section.</p>
]]></content></item><item><title>How to use service accounts for Kubernetes imagePullSecrets</title><link>https://rizwan-kh.github.io/posts/2020/01/how-to-use-service-accounts-for-kubernetes-imagepullsecrets/</link><pubDate>Thu, 02 Jan 2020 17:40:27 +0400</pubDate><guid>https://rizwan-kh.github.io/posts/2020/01/how-to-use-service-accounts-for-kubernetes-imagepullsecrets/</guid><description>What are Service Accounts in Kubernetes? As per Kubernetes.io - A service account provides an identity for processes that run in a Pod.
One can think of service accounts as service users for pods. They help pods authenticate with the api-server and interact with it.
Many times, we come across a situation where our organization uses a private Docker registry to store the Docker images and to make this available we need to create a docker-registry kubernetes secret and pass as imagePullSecrets in the deployment manifest.</description><content type="html"><![CDATA[<p><img src="/kubernetes.jpg" alt="Kubernetes"></p>
<h2 id="what-are-service-accounts-in-kubernetes">What are Service Accounts in Kubernetes?</h2>
<p>As per <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/">Kubernetes.io</a> - A service account provides an identity for processes that run in a Pod.</p>
<p>One can think of service accounts as service users for pods. They help pods authenticate with the api-server and interact with it.</p>
<h2 id="heading"></h2>
<p>Many times, we come across a situation where our organization uses a private Docker registry to store the Docker images and to make this available we need to create a <code>docker-registry</code> kubernetes secret and pass as <code>imagePullSecrets</code> in the deployment manifest.</p>
<pre tabindex="0"><code>kubectl create secret docker-registry registry-cred \
 --docker-server=my.private-registry.com \
 --docker-username=my_username \
 --docker-password=&#34;my_superr_strong_password&#34; \
 --docker-email=my.email@mycompany.com -n my-namespace
</code></pre><p>Then, we pass this secret in the deployment manifest as below.</p>
<pre tabindex="0"><code>...
      imagePullSecrets:
      - name: registry-cred
...
</code></pre><h2 id="the-problem-with-this-approach">The problem with this approach?</h2>
<p>Not many that I can think of, except below ones:</p>
<ul>
<li>The deployment yaml are generally developed by Developers who doesn&rsquo;t need to know about this credentials</li>
<li>If there are a large number of pods in the namespace, then each manifest needs to be updated, whenever the password is rotated</li>
</ul>
<h2 id="the-solution">The solution</h2>
<p><code>serviceAccounts</code> - your Kubernetes administrator can just patch serviceAccounts with the registry credential secret and you don&rsquo;t need to worry about replacing or adding it in your manifest yaml each time.</p>
<pre tabindex="0"><code>kubectl patch serviceaccount default \
-p &#39;{&#34;imagePullSecrets&#34;: [{&#34;name&#34;: &#34;registry-cred&#34;}]}&#39; -n my-namespace
</code></pre>]]></content></item><item><title>Docker Cheat Sheet</title><link>https://rizwan-kh.github.io/posts/2019/10/docker-cheat-sheet/</link><pubDate>Fri, 11 Oct 2019 05:11:13 +0400</pubDate><guid>https://rizwan-kh.github.io/posts/2019/10/docker-cheat-sheet/</guid><description>Below are few of the main and basic commands used in Docker, an easy pick-up and good-to-go command page for docker troubleshooting.
Alias If you are a lazy developer/sysadmin like me, the first thing you should do on your system is to make easy alias of all the long commands, below are the ones I often use on any system I use on a daily basis:
These can be imported on ~/.</description><content type="html"><![CDATA[<p><img src="/docker.png" alt="docker">
Below are few of the main and basic commands used in Docker, an easy pick-up and good-to-go command page for docker troubleshooting.</p>
<h2 id="alias">Alias</h2>
<hr>
<p>If you are a lazy developer/sysadmin like me, the first thing you should do on your system is to make easy alias of all the long commands, below are the ones I often use on any system I use on a  daily basis:</p>
<p>These can be imported on ~/.bashrc (if you use bash) or ~/.zshrc (if you are a MAC user and use ZSH)</p>
<script type="application/javascript" src="https://gist.github.com/rizwan-kh/032e587751e54a2fd26f44c0267ea5c5.js"></script>

<h3 id="commands-and-their-usage">Commands and their usage</h3>
<p>Mostly used commands are aliased above, but to explain what each does, please read on</p>
<pre tabindex="0"><code># to build a docker image with a certain name and certain tag, use the below Docker build comamnd
docker build --tag imagename:tagname --file /path/to/Dockerfile


# to check the docker images
docker images


# to run a docker container in detach mode publishing hostport:containerport and mounting a host vol to container vol, giving a name to the runnging container and the hostname to container
docker run --tty --interactive --publish 2222:22 --hostname my-x-host --volume /hostvolume:/containervol --name name-of-running-container --detach imagename:tagname
# docker run -ti -p 2222:22 -h my-x-host -v /hostvolume:/containervol -n name-of-running-container -d imagename:tagname (this is shorter version of the above command)


# to see all running containers
docker ps


# to see all containers (running, stopped, exited, etc.)
docker ps -a


# to get inside a running container
docker exec -ti CONTAINERNAME/CONTAINERID bash


# to stop a running container
docker stop CONTAINERNAME/CONTAINERID


# to remove a stopped container
docker rm CONTAINERNAME/CONTAINERID

# to see logs from containers
docker logs CONTAINERNAME/CONTAINERID
</code></pre>]]></content></item><item><title>tmux</title><link>https://rizwan-kh.github.io/posts/2019/06/tmux/</link><pubDate>Thu, 13 Jun 2019 21:53:35 +0400</pubDate><guid>https://rizwan-kh.github.io/posts/2019/06/tmux/</guid><description>tmux - Terminal Multiplexer tmux is a terminal multiplexer. It helps switch between multiple programs in one terminal, detach them(they keep running in background) and reattach when needed. I started using tmux as someone recommended me to start using it as it helps with CKA &amp;amp; CKAD (which I have still not attempted) and then later on I was heavily using it for my website(with Hugo). I had previously used mPutty, but mPutty uses mutliple logged in sessions to display on screen, whereas tmux simply multiply the existing sessions on screen.</description><content type="html"><![CDATA[<p><img src="/tmux-logo.png" alt="tmux"></p>
<h1 id="tmux---terminal-multiplexer">tmux - Terminal Multiplexer</h1>
<p>tmux is a terminal multiplexer. It helps switch between multiple programs in one terminal, detach them(they keep running in background) and reattach when needed. I started using tmux as someone recommended me to start using it as it helps with CKA &amp; CKAD (which I  have still not attempted) and then later on I was heavily using it for my website(with Hugo). I had previously used mPutty, but mPutty uses mutliple logged in sessions to display on screen, whereas tmux simply multiply the existing sessions on screen.</p>
<h3 id="installation">Installation</h3>
<p>Major distribution of linux provides tmux packages via standard pre-built packages of tmux.</p>
<table>
<thead>
<tr>
<th>Platform</th>
<th>Install Command</th>
</tr>
</thead>
<tbody>
<tr>
<td>Debian or Ubuntu</td>
<td><code>apt install tmux</code></td>
</tr>
<tr>
<td>RHEL or CentOS</td>
<td><code>yum install tmux</code></td>
</tr>
<tr>
<td>macOS (using Homebrew)</td>
<td><code>brew install tmux</code></td>
</tr>
</tbody>
</table>
<h3 id="basic-commands-and-usage">Basic commands and usage</h3>
<table>
<thead>
<tr>
<th style="text-align:left">Command</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">tmux</td>
<td style="text-align:left">start a new session</td>
</tr>
<tr>
<td style="text-align:left">tmux new -s my-kube-session</td>
<td style="text-align:left">start a new session with name</td>
</tr>
<tr>
<td style="text-align:left">tmux a</td>
<td style="text-align:left">attach</td>
</tr>
<tr>
<td style="text-align:left">tmux a  -t my-kube-session</td>
<td style="text-align:left">attach to a named session</td>
</tr>
<tr>
<td style="text-align:left">tmux ls</td>
<td style="text-align:left">list all tmux sessions</td>
</tr>
<tr>
<td style="text-align:left">tmux kill-session -t my-kube-session</td>
<td style="text-align:left">kill the session named my-kube-session</td>
</tr>
</tbody>
</table>
<p>After a session is created, inside to perform any action, we need to hit <code>ctrl+b</code> followed by any below command/keystroke</p>
<table>
<thead>
<tr>
<th style="text-align:left">Keystroke</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">ctrl+b -&gt; c</td>
<td style="text-align:left">create new shell</td>
</tr>
<tr>
<td style="text-align:left">ctrl+b -&gt; n</td>
<td style="text-align:left">next shell</td>
</tr>
<tr>
<td style="text-align:left">ctrl+b -&gt; p</td>
<td style="text-align:left">previous shell</td>
</tr>
<tr>
<td style="text-align:left">ctrl+b -&gt; d</td>
<td style="text-align:left">detach session</td>
</tr>
<tr>
<td style="text-align:left">ctrl+b -&gt; %</td>
<td style="text-align:left">vertical split</td>
</tr>
<tr>
<td style="text-align:left">ctrl+b -&gt; &quot;</td>
<td style="text-align:left">horizontal split</td>
</tr>
<tr>
<td style="text-align:left">ctrl+b -&gt; [arrow keys]</td>
<td style="text-align:left">to switch between the vertical/horizontal panes</td>
</tr>
<tr>
<td style="text-align:left">ctrl+b -&gt; z</td>
<td style="text-align:left">zoom in and zoom out between split panes</td>
</tr>
</tbody>
</table>
<p>That&rsquo;s all more or less the keystroke you need to know; It comes of naturally when you start using it after a few days but until then - you can refer these as needed.</p>
]]></content></item></channel></rss>